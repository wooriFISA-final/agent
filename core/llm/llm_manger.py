"""
LLM Manager Module
LLM ì„¤ì • ë° ê´€ë¦¬ (Ollama HTTP API ì§ì ‘ í˜¸ì¶œ)
"""
from typing import Optional, Dict, Any, List
import requests
from core.logging.logger import setup_logger
from core.config.setting import settings

logger = setup_logger()


class LLMManager:
    """
    LLM ê´€ë¦¬ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤)
    Ollama Chat APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ë°©ì‹
    """
    
    _instance: Optional['LLMManager'] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def get_default_config(cls) -> Dict[str, Any]:
        """
        ì „ì—­ ê¸°ë³¸ LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸° (settings ê¸°ë°˜)
        
        Returns:
            ê¸°ë³¸ LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        return {
            "base_url": str(settings.LLM_API_BASE_URL),
            "provider": settings.LLM_PROVIDER,
            "model": settings.LLM_MODEL,
            "temperature": settings.LLM_TEMPERATURE,
            "top_p": settings.LLM_TOP_P,
            "top_k": settings.LLM_TOP_K,
            "num_ctx": settings.LLM_NUM_CTX,
            "stream": settings.LLM_STREAM,
            "format": settings.LLM_FORMAT,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @classmethod
    def merge_config(cls, **overrides) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ì„¤ì •ê³¼ ì˜¤ë²„ë¼ì´ë“œ ë³‘í•©
        
        Args:
            **overrides: ë®ì–´ì“¸ ì„¤ì •ê°’ë“¤
            
        Returns:
            ë³‘í•©ëœ LLM ì„¤ì •
        """
        config = cls.get_default_config()
        
        # overridesì— ìˆëŠ” ê°’ë§Œ ì—…ë°ì´íŠ¸
        for key in config.keys():
            if key in overrides and overrides[key] is not None:
                config[key] = overrides[key]
        
        logger.debug(f"ğŸ¤– Merged LLM Config: {config}")
        return config
    
    @classmethod
    def test_connection(cls, **config_overrides) -> bool:
        """
        LLM ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Args:
            **config_overrides: í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            config = cls.merge_config(**config_overrides)
            response = cls._call_ollama_chat(
                messages=[{"role": "user", "content": "Hello"}],
                model=config["model"],
                base_url=config["base_url"],
                timeout=10,
                temperature=config["temperature"],
                top_k=config["top_k"],
                top_p=config["top_p"],
                num_ctx=config["num_ctx"],
                stream=False
            )
            logger.info(f"âœ… LLM connection test successful: {config['base_url']}")
            return True
        except Exception as e:
            logger.error(f"âŒ LLM connection test failed: {e}")
            return False
    
    @classmethod
    def _call_ollama_chat(
        cls,
        messages: List[Dict[str, str]],
        model: str,
        base_url: str,
        timeout: int = 180,
        stream: bool = False,
        format: str = "",
        **kwargs  # âœ… ì¶”ê°€ íŒŒë¼ë¯¸í„° ë°›ê¸°
    ) -> str:
        """
        API í˜¸ì¶œ 
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user/assistant/system", "content": "..."}]
            model: ëª¨ë¸ ì´ë¦„
            base_url: Ollama ì„œë²„ URL
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            format: ì‘ë‹µ í¬ë§· (json ë“±)
            **kwargs: temperature, top_k, top_p, num_ctx ë“±
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        # âœ… options ê°ì²´ ìƒì„±
        options = {}
        ollama_option_keys = [
            'temperature', 'top_k', 'top_p', 'min_p',
            'num_ctx', 'num_predict', 'seed', 'stop'
        ]
        
        for key in ollama_option_keys:
            if key in kwargs and kwargs[key] is not None:
                options[key] = kwargs[key]
        
        # Payload êµ¬ì„±
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        # optionsê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if options:
            payload["options"] = options
        
        # formatì´ ìˆìœ¼ë©´ ì¶”ê°€
        if format:
            payload["format"] = format
        
        logger.debug(f"ğŸ¤– Ollama API Request: {payload}")
        
        try:
            response = requests.post(
                f"{base_url}/chat/completions",
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            
            result = response.json()
            logger.debug(f"Ollama API result: {result}")
            
            # stream=Falseì¸ ê²½ìš° message.content ë°˜í™˜
            if not stream:
                content = (
                    result.get("message", {}).get("content")  # Ollama ê¸°ë³¸ êµ¬ì¡°
                    or (
                        result.get("choices", [{}])[0]  # OpenAI í˜¸í™˜ êµ¬ì¡°
                        .get("message", {})
                        .get("content")
                    )
                    or ""
                )
                return content.strip()
            
            # stream=Trueì¸ ê²½ìš°ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
            return result
            
        except requests.exceptions.Timeout:
            error_msg = f"âŒ Ollama íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ ì´ˆê³¼). ëª¨ë¸: {model}, URL: {base_url}"
            logger.error(error_msg)
            raise TimeoutError(error_msg)
            
        except requests.exceptions.ConnectionError:
            error_msg = f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {base_url}"
            logger.error(error_msg)
            raise ConnectionError(error_msg)
            
        except requests.exceptions.RequestException as e:
            error_msg = f"âŒ Ollama API í˜¸ì¶œ ì˜¤ë¥˜: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)


class LLMHelper:
    """LLM ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤"""
    
    @staticmethod
    def invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ê°„ë‹¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        config = LLMManager.merge_config(**kwargs)
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        return LLMManager._call_ollama_chat(
            messages=messages,
            model=config["model"],
            base_url=config["base_url"],
            timeout=config["timeout"],
            stream=kwargs.get("stream", False),
            format=kwargs.get("format", ""),
            # âœ… options ëŒ€ì‹  ê°œë³„ íŒŒë¼ë¯¸í„° ì „ë‹¬
            temperature=kwargs.get("temperature", config["temperature"]),
            top_k=kwargs.get("top_k", config["top_k"]),
            top_p=kwargs.get("top_p", config["top_p"]),
            num_ctx=kwargs.get("num_ctx", config["num_ctx"])
        )
    
    @staticmethod
    def invoke_with_history(
        history: List[Dict[str, str]],
        **kwargs
    ) -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: í˜„ì¬ í”„ë¡¬í”„íŠ¸
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant/system", "content": "..."}]
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Returns:
            LLM ì‘ë‹µ
        """
        config = LLMManager.merge_config(**kwargs)
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        messages.extend(history)
        
        # # í˜„ì¬ í”„ë¡¬í”„íŠ¸
        # if prompt:
        #     messages.append({"role": "user", "content": prompt})
        
        return LLMManager._call_ollama_chat(
            messages=messages,
            model=config["model"],
            base_url=config["base_url"],
            timeout=config["timeout"],
            stream=kwargs.get("stream", False),
            format=kwargs.get("format", ""),
            # âœ… options ëŒ€ì‹  ê°œë³„ íŒŒë¼ë¯¸í„° ì „ë‹¬
            temperature=kwargs.get("temperature", config["temperature"]),
            top_k=kwargs.get("top_k", config["top_k"]),
            top_p=kwargs.get("top_p", config["top_p"]),
            num_ctx=kwargs.get("num_ctx", config["num_ctx"])
        )
    
    @staticmethod
    def stream_invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Yields:
            ì‘ë‹µ ì²­í¬
        """
        config = LLMManager.merge_config(**kwargs)
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # âœ… options ê°ì²´ ìƒì„±
        options = {
            "temperature": kwargs.get("temperature", config["temperature"]),
            "top_k": kwargs.get("top_k", config["top_k"]),
            "top_p": kwargs.get("top_p", config["top_p"]),
            "num_ctx": kwargs.get("num_ctx", config["num_ctx"])
        }
        
        payload = {
            "model": config["model"],
            "messages": messages,
            "stream": True,
            "options": options
        }
        
        if kwargs.get("format"):
            payload["format"] = kwargs["format"]
        
        try:
            response = requests.post(
                f"{config['base_url']}/chat/completions",
                json=payload,
                timeout=config["timeout"],
                stream=True
            )
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    import json
                    chunk = json.loads(line)
                    message = chunk.get('message', {})
                    content = message.get('content', '')
                    if content:
                        yield content
                        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            raise