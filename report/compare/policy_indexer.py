# report_project/compare/policy_indexer.py

import os
import faiss
import json
import numpy as np
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer

# ğŸš¨ Policy í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜ ì„í¬íŠ¸
from .rag_search_engine import get_policy_chapters 

# --- ì„¤ì • ---
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
CACHE_FILE_NAME = 'policy_cache.json'
FAISS_INDEX_FILE = 'policy_faiss.index'

# --- ê²½ë¡œ ì„¤ì • ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(CURRENT_DIR, 'data')
CACHE_PATH = os.path.join(DATA_DIR, CACHE_FILE_NAME)
FAISS_PATH = os.path.join(DATA_DIR, FAISS_INDEX_FILE)

# ğŸš¨ [ìˆ˜ì • ë°˜ì˜] íŒŒì¼ëª… ë‹¨ìˆœí™” (ì‚¬ìš©ìë‹˜ì˜ ì‹œìŠ¤í…œì— ë§ì¶¤)
POLICY_PATH_OLD = os.path.join(DATA_DIR, "20241224.pdf")
POLICY_PATH_NEW = os.path.join(DATA_DIR, "20250305.pdf")

def create_policy_index():
    """
    ë‘ ì •ì±… PDFë¥¼ ì½ì–´ ì²­í¬ë¥¼ ë¶„í• í•˜ê³ , FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„± í›„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # 1. ëª¨ë¸ ë¡œë“œ
    print("â³ 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: SentenceTransformer ë¡œë“œ ì‹¤íŒ¨. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë˜ëŠ” ì¸í„°ë„· ì—°ê²° í™•ì¸ í•„ìš”. ì˜¤ë¥˜: {e}")
        return

    # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„í• 
    print("â³ 2. ì •ì±… íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í¬ ë¶„í• ...")
    
    # ë‘ íŒŒì¼ì—ì„œ ì²­í¬ ì¶”ì¶œ
    chapters_old = get_policy_chapters(POLICY_PATH_OLD)
    chapters_new = get_policy_chapters(POLICY_PATH_NEW)

    # ğŸš¨ DB ì €ì¥ì„ ìœ„í•œ í†µí•© ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    all_chunks = []
    
    for i, chap in enumerate(chapters_old):
        all_chunks.append({
            "id": f"OLD_{i}",
            "version": "20241224",
            "title": chap['title'],
            "content": chap['content']
        })

    for i, chap in enumerate(chapters_new):
        all_chunks.append({
            "id": f"NEW_{i}",
            "version": "20250305",
            "title": chap['title'],
            "content": chap['content']
        })
    
    if not all_chunks:
        print("âŒ ì˜¤ë¥˜: PDFì—ì„œ ìœ íš¨í•œ ì²­í¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (PDF íŒŒì¼ ë˜ëŠ” rag_search_engine.py í™•ì¸ í•„ìš”)")
        return

    # 3. ì„ë² ë”© ë²¡í„° ìƒì„±
    documents = [c['content'] for c in all_chunks]
    print(f"â³ 3. ì´ {len(documents)}ê°œ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    # ğŸš¨ sentences_transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ìƒì„±
    embeddings = model.encode(documents, convert_to_numpy=True)
    d = embeddings.shape[1] # ë²¡í„° ì°¨ì› ìˆ˜ (ì˜ˆ: 384)
    
    # 4. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì €ì¥
    index = faiss.IndexFlatL2(d) # L2 ê±°ë¦¬ ê¸°ë°˜ì˜ í‰ë©´ ì¸ë±ìŠ¤ ìƒì„±
    index.add(embeddings.astype('float32')) # FAISSëŠ” float32ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    
    # 5. FAISS ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥
    faiss.write_index(index, FAISS_PATH)
    print(f"âœ… 4. FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ. ({FAISS_PATH})")

    # 6. ì›ë³¸ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ìºì‹œ íŒŒì¼ ì €ì¥ (FAISSëŠ” í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•„ìˆ˜)
    with open(CACHE_PATH, 'w', encoding='utf-8') as f:
        json.dump(all_chunks, f, indent=2, ensure_ascii=False)
    print(f"âœ… 5. ì›ë³¸ í…ìŠ¤íŠ¸ ìºì‹œ ì €ì¥ ì™„ë£Œ. ({CACHE_PATH})")
    
    print("\n--- ì¸ë±ì‹± ì™„ë£Œ: RAG ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ ---")


if __name__ == '__main__':
    create_policy_index()