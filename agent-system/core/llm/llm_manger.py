"""
LLM Manager Module
LLM ì„¤ì • ë° ê´€ë¦¬ (Ollama, Anthropic, OpenAI ì§€ì›)
"""
from typing import Optional, Dict, Any, List
from langchain_ollama import ChatOllama
# from langchain_anthropic import ChatAnthropic
# from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from core.config.setting import Settings
import logging

logger = logging.getLogger(__name__)


class LLMManager:
    """
    LLM ê´€ë¦¬ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤)
    
    ì§€ì› Provider:
    - Ollama (ë¡œì»¬)
    - Anthropic (Claude)
    - OpenAI (GPT)
    """
    
    _instance: Optional['LLMManager'] = None
    _llm: Optional[BaseChatModel] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def get_llm(cls, **kwargs) -> BaseChatModel:
        """
        LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì¸ìŠ¤í„´ìŠ¤
        """
        if cls._llm is None:
            cls._llm = cls._create_llm(**kwargs)
        return cls._llm
    
    @classmethod
    def _create_llm(cls, **kwargs) -> BaseChatModel:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        config = Settings.get_config()
        
        provider = kwargs.get("provider") or config.llm_provider
        model = kwargs.get("model") or config.llm_model
        temperature = kwargs.get("temperature") or config.llm_temperature
        
        # providerê°€ ollamaê°€ ì•„ë‹ˆê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ollama ì‚¬ìš©
        if provider.lower() != "ollama":
            logger.warning(f"âš ï¸ Provider '{provider}' is not supported or not configured. Using Ollama as default.")
            provider = "ollama"
            # ollama ê¸°ë³¸ ëª¨ë¸ ì„¤ì • (kwargsì— modelì´ ì—†ê±°ë‚˜ config ëª¨ë¸ê³¼ ê°™ìœ¼ë©´)
            if not kwargs.get("model") or model == config.llm_model:
                model = "qwen3:8b"  # ollama ê¸°ë³¸ ëª¨ë¸
        
        logger.info(f"ğŸ¤– Creating LLM: provider={provider}, model={model}")
        
        if provider.lower() == "ollama":
            # kwargsì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•œ ì¸ìë“¤ ì œê±°
            filtered_kwargs = {k: v for k, v in kwargs.items() 
                             if k not in ["provider", "model", "temperature"]}
            return cls._create_ollama(model, temperature, **filtered_kwargs)
        # elif provider.lower() == "anthropic":
        #     return cls._create_anthropic(model, temperature, **kwargs)
        # elif provider.lower() == "openai":
        #     return cls._create_openai(model, temperature, **kwargs)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
    
    @classmethod
    def _create_ollama(
        cls, 
        model: str, 
        temperature: float,
        **kwargs
    ) -> ChatOllama:
        """
        Ollama LLM ìƒì„±
        
        ê¸°ë³¸ ì„¤ì •:
        - base_url: http://localhost:11434
        - model: ì„¤ì • ëª¨ë¸
        """
        base_url = kwargs.get("base_url", "http://localhost:11434")
        
        llm = ChatOllama(
            model=model,
            temperature=temperature,
            base_url=base_url,
            **{k: v for k, v in kwargs.items() 
               if k not in ["provider", "model", "temperature", "base_url"]}
        )
        
        logger.info(f"âœ… Ollama LLM created: {model} at {base_url}")
        return llm
    
    # @classmethod
    # def _create_anthropic(
    #     cls,
    #     model: str,
    #     temperature: float,
    #     **kwargs
    # ) -> ChatAnthropic:
    #     """Anthropic Claude LLM ìƒì„±"""
    #     config = Settings.get_config()
    #     api_key = kwargs.get("api_key") or config.llm_api_key
        
    #     if not api_key:
    #         raise ValueError("Anthropic API key is required")
        
    #     llm = ChatAnthropic(
    #         model=model,
    #         temperature=temperature,
    #         api_key=api_key,
    #         **{k: v for k, v in kwargs.items() 
    #            if k not in ["provider", "model", "temperature", "api_key"]}
    #     )
        
    #     logger.info(f"âœ… Anthropic LLM created: {model}")
    #     return llm
    
    # @classmethod
    # def _create_openai(
    #     cls,
    #     model: str,
    #     temperature: float,
    #     **kwargs
    # ) -> ChatOpenAI:
    #     """OpenAI GPT LLM ìƒì„±"""
    #     config = Settings.get_config()
    #     api_key = kwargs.get("api_key") or config.llm_api_key
        
    #     if not api_key:
    #         raise ValueError("OpenAI API key is required")
        
    #     llm = ChatOpenAI(
    #         model=model,
    #         temperature=temperature,
    #         api_key=api_key,
    #         **{k: v for k, v in kwargs.items() 
    #            if k not in ["provider", "model", "temperature", "api_key"]}
    #     )
        
    #     logger.info(f"âœ… OpenAI LLM created: {model}")
    #     return llm
    
    @classmethod
    def reset(cls):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (ì¬ìƒì„± ì‹œ ì‚¬ìš©)"""
        cls._llm = None
        logger.info("ğŸ”„ LLM instance reset")
    
    @classmethod
    async def test_connection(cls) -> bool:
        """
        LLM ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            llm = cls.get_llm()
            response = await llm.ainvoke([HumanMessage(content="Hello")])
            logger.info(f"âœ… LLM connection test successful: {response.content[:50]}...")
            return True
        except Exception as e:
            logger.error(f"âŒ LLM connection test failed: {e}")
            return False


class LLMHelper:
    """LLM ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤"""
    
    @staticmethod
    async def invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ê°„ë‹¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        llm = LLMManager.get_llm(**kwargs)
        
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        response = await llm.ainvoke(messages)
        return response.content
    
    @staticmethod
    async def invoke_with_history(
        prompt: str,
        history: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: í˜„ì¬ í”„ë¡¬í”„íŠ¸
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant", "content": "..."}]
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Returns:
            LLM ì‘ë‹µ
        """
        llm = LLMManager.get_llm(**kwargs)
        
        messages = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))
        
        # í˜„ì¬ í”„ë¡¬í”„íŠ¸
        messages.append(HumanMessage(content=prompt))
        
        response = await llm.ainvoke(messages)
        return response.content
    
    @staticmethod
    async def stream_invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Yields:
            ì‘ë‹µ ì²­í¬
        """
        llm = LLMManager.get_llm(**kwargs)
        
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        async for chunk in llm.astream(messages):
            yield chunk.content