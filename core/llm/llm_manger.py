"""
LLM Manager Module
LLM ì„¤ì • ë° ê´€ë¦¬ (AWS Bedrock Converse API)
"""
from typing import Optional, Dict, Any, List, Tuple, Union
import boto3
from botocore.exceptions import ClientError
from core.logging.logger import setup_logger
from core.config.setting import settings

logger = setup_logger()


class LLMManager:
    """
    LLM ê´€ë¦¬ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤)
    AWS Bedrock Converse API ì‚¬ìš©
    """
    
    _instance: Optional['LLMManager'] = None
    _bedrock_client = None  # boto3 í´ë¼ì´ì–¸íŠ¸ ìºì‹œ
    _current_region = None  # í˜„ì¬ ì„¤ì •ëœ ë¦¬ì „
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def _get_bedrock_client(cls, region: str):
        """
        boto3 Bedrock í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)
        
        Args:
            region: AWS ë¦¬ì „
            
        Returns:
            boto3 Bedrock Runtime í´ë¼ì´ì–¸íŠ¸
        """
        # ë¦¬ì „ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if cls._bedrock_client is None or cls._current_region != region:
            logger.info(f"ìƒˆë¡œìš´ Bedrock í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë¦¬ì „: {region}")
            cls._bedrock_client = boto3.client(
                service_name="bedrock-runtime",
                region_name=region
            )
            cls._current_region = region
            logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ìƒì„±ë˜ê³  ìºì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return cls._bedrock_client
    
    @classmethod
    def get_default_config(cls) -> Dict[str, Any]:
        """
        ì „ì—­ ê¸°ë³¸ LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸° (AWS Bedrock ê¸°ë°˜)
        
        Returns:
            ê¸°ë³¸ LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        return {
            "region": str(settings.AWS_REGION),
            "model_id": settings.BEDROCK_MODEL_ID,
            "temperature": settings.LLM_TEMPERATURE,
            "top_p": settings.LLM_TOP_P,
            "max_tokens": settings.LLM_MAX_TOKENS,
            "stream": settings.LLM_STREAM,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @classmethod
    def merge_config(cls, **overrides) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ì„¤ì •ê³¼ ì˜¤ë²„ë¼ì´ë“œ ë³‘í•©
        
        Args:
            **overrides: ë®ì–´ì“¸ ì„¤ì •ê°’ë“¤
            
        Returns:
            ë³‘í•©ëœ LLM ì„¤ì •
        """
        config = cls.get_default_config()
        
        # overridesì— ìˆëŠ” ê°’ë§Œ ì—…ë°ì´íŠ¸
        for key in config.keys():
            if key in overrides and overrides[key] is not None:
                config[key] = overrides[key]
        
        logger.debug(f"ë³‘í•©ëœ LLM ì„¤ì •: {config}")
        return config
    
    @classmethod
    def test_connection(cls, **config_overrides) -> bool:
        """
        LLM ì—°ê²° í…ŒìŠ¤íŠ¸ (Bedrock)
        
        Args:
            **config_overrides: í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            config = cls.merge_config(**config_overrides)
            response = cls._call_bedrock_converse(
                messages=[{"role": "user", "content": "Hello"}],
                model_id=config["model_id"],
                region=config["region"],
                timeout=10,
                temperature=config["temperature"],
                top_p=config["top_p"],
                max_tokens=config.get("max_tokens", 10000)
            )
            logger.info(f"Bedrock ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: region={config['region']}, model={config['model_id']}")
            return True
        except Exception as e:
            logger.error(f"Bedrock ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    @classmethod
    def _prepare_bedrock_messages(
        cls,
        messages: List[Dict[str, str]]
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        Ollama í¬ë§· ë©”ì‹œì§€ë¥¼ Bedrock Converse í¬ë§·ìœ¼ë¡œ ë³€í™˜
        
        Args:
            messages: Ollama í¬ë§· ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Tuple[system_messages, conversation_messages]
        """
        system_messages = []
        conversation_messages = []
        
        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")
            
            if role == "system":
                # System ë©”ì‹œì§€ëŠ” ë³„ë„ ë°°ì—´ë¡œ
                system_messages.append({"text": content})
                
            elif role in ["user", "assistant"]:
                # contentê°€ ì´ë¯¸ Bedrock í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                # (ì˜ˆ: [{"toolUse": {...}}, {"text": "..."}])
                if isinstance(content, list) and content and isinstance(content[0], dict):
                    # reasoningContent ë¸”ë¡ í•„í„°ë§ (Extended Thinking ëª¨ë¸ìš©)
                    # toolUse, text, image ë“±ë§Œ ìœ ì§€
                    filtered_content = [
                        block for block in content 
                        if not isinstance(block, dict) or "reasoningContent" not in block
                    ]
                    
                    # í•„í„°ë§ í›„ contentê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ê°€
                    if not filtered_content:
                        filtered_content = [{"text": ""}]
                    
                    # ì´ë¯¸ Bedrock í˜•ì‹ì´ë©´ í•„í„°ë§ëœ content ì‚¬ìš©
                    conversation_messages.append({
                        "role": role,
                        "content": filtered_content
                    })
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
                    conversation_messages.append({
                        "role": role,
                        "content": [{"text": str(content)}]
                    })
                
            elif role == "tool":
                # ToolMessage ì²˜ë¦¬
                # LangChain ToolMessageì—ì„œ tool_call_idì™€ content ì¶”ì¶œ
                tool_use_id = msg.get("tool_call_id", "unknown")
                
                # Tool ê²°ê³¼ë¥¼ íŒŒì‹± ì‹œë„ (JSONì¸ì§€ í™•ì¸)
                try:
                    import json
                    result_json = json.loads(content)
                    tool_content = [{"json": result_json}]
                except:
                    # JSONì´ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                    tool_content = [{"text": content}]
                
                # toolResult ë¸”ë¡ ìƒì„±
                tool_result_block = {
                    "toolResult": {
                        "toolUseId": tool_use_id,
                        "content": tool_content
                    }
                }
                
                # ì´ì „ ë©”ì‹œì§€ê°€ toolResultë¥¼ í¬í•¨í•˜ëŠ” user ë©”ì‹œì§€ì¸ì§€ í™•ì¸
                if (conversation_messages and 
                    conversation_messages[-1]["role"] == "user" and 
                    "toolResult" in conversation_messages[-1]["content"][0]):
                    
                    # ê¸°ì¡´ ë©”ì‹œì§€ì— ì¶”ê°€
                    conversation_messages[-1]["content"].append(tool_result_block)
                else:
                    # ìƒˆë¡œìš´ user ë©”ì‹œì§€ ìƒì„±
                    conversation_messages.append({
                        "role": "user",
                        "content": [tool_result_block]
                    })
        
        return system_messages, conversation_messages
    
    @classmethod
    def _handle_tool_response(cls, response: Dict) -> Tuple[bool, Optional[Dict]]:
        """
        Bedrock ì‘ë‹µì—ì„œ tool_use í™•ì¸ ë° ì •ë³´ ì¶”ì¶œ
        
        Args:
            response: Bedrock converse() ì‘ë‹µ
            
        Returns:
            Tuple[is_tool_requested, tool_request_info]
            - is_tool_requested: Toolì´ ìš”ì²­ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
            - tool_request_info: Tool ìš”ì²­ ì •ë³´ (toolUseId, name, input)
        """
        stop_reason = response.get("stopReason")
        
        if stop_reason == "tool_use":
            # Tool ìš”ì²­ ì¶”ì¶œ
            message = response.get("output", {}).get("message", {})
            content = message.get("content", [])
            
            for block in content:
                if "toolUse" in block:
                    tool_use = block["toolUse"]
                    return True, {
                        "tool_use_id": tool_use["toolUseId"],
                        "tool_name": tool_use["name"],
                        "tool_input": tool_use.get("input", {})
                    }
        
        return False, None
    
    @classmethod
    def _call_bedrock_converse(
        cls,
        messages: List[Dict[str, str]],
        model_id: str,
        region: str,
        timeout: int = 180,
        tool_config: Optional[Dict] = None,
        tool_choice: Optional[Union[str, Dict]] = None,
        **kwargs
    ) -> Dict:
        """
        AWS Bedrock Converse API í˜¸ì¶œ
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            model_id: Bedrock ëª¨ë¸ ID
            region: AWS ë¦¬ì „
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            tool_config: Bedrock toolConfig (ì„ íƒ)
            tool_choice: Bedrock toolChoice (ì„ íƒ, ì˜ˆ: {"any": {}}, {"auto": {}}, {"tool": {"name": "tool_name"}})
            **kwargs: temperature, top_p, max_tokens ë“±
            
        Returns:
            Dict: ì „ì²´ Bedrock ì‘ë‹µ (stopReason, output, usage ë“± í¬í•¨)
        """
        # ë©”ì‹œì§€ ë³€í™˜
        system_messages, conversation_messages = cls._prepare_bedrock_messages(messages)
        
        logger.info(f"Bedrock API í˜¸ì¶œ ì¤€ë¹„")
        logger.info(f"   Region: {region}")
        logger.info(f"   Model ID: {model_id}")
        logger.info(f"   System messages: {len(system_messages)}")
        logger.info(f"   Conversation messages: {len(conversation_messages)}")
        
        # AWS í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        import os
        logger.info(f"AWS_BEARER_TOKEN_BEDROCK: {'ì„¤ì •ë¨' if os.getenv('AWS_BEARER_TOKEN_BEDROCK') else 'ì—†ìŒ'}")
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)
        client = cls._get_bedrock_client(region)
        
        # inferenceConfig êµ¬ì„±
        inference_config = {}
        if "temperature" in kwargs:
            inference_config["temperature"] = kwargs["temperature"]
        if "top_p" in kwargs:
            inference_config["topP"] = kwargs["top_p"]
        if "max_tokens" in kwargs:
            inference_config["maxTokens"] = kwargs["max_tokens"]
        
        # API ìš”ì²­ íŒŒë¼ë¯¸í„°
        request_params = {
            "modelId": model_id,
            "messages": conversation_messages
        }
        
        if system_messages:
            request_params["system"] = system_messages
        
        # toolConfig ì¶”ê°€
        if tool_config:
            request_params["toolConfig"] = tool_config
            logger.info(f"âœ… toolConfig ì¶”ê°€: {len(tool_config.get('tools', []))}ê°œì˜ ë„êµ¬")
            
            # toolChoice ì¶”ê°€ (toolConfigê°€ ìˆì„ ë•Œë§Œ)
            if tool_choice:
                request_params["toolConfig"]["toolChoice"] = tool_choice
                logger.info(f"âœ… toolChoice ì¶”ê°€: {tool_choice}")
        
        if inference_config:
            request_params["inferenceConfig"] = inference_config
        
        logger.debug(f"ìš”ì²­ íŒŒë¼ë¯¸í„° í‚¤: {list(request_params.keys())}")
        
        try:
            logger.info("Bedrock API í˜¸ì¶œ ì‹œë„...")
            response = client.converse(**request_params)
            logger.info("Bedrock API í˜¸ì¶œ ì„±ê³µ")
            logger.debug(f"ì‘ë‹µ í‚¤: {list(response.keys())}")
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
            usage = response.get("usage", {})
            input_tokens = usage.get("inputTokens", 0)
            output_tokens = usage.get("outputTokens", 0)
            total_tokens = usage.get("totalTokens", 0)
            logger.info(f"ğŸ“Š Token Usage - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}")
            
            # ì „ì²´ ì‘ë‹µ ë°˜í™˜ (stopReason í¬í•¨)
            return response
            
        except ClientError as e:
            logger.error(f"Bedrock ClientError:")
            logger.error(f"   Error Code: {e.response['Error']['Code']}")
            logger.error(f"   Error Message: {e.response['Error']['Message']}")
            logger.error(f"   HTTP Status: {e.response['ResponseMetadata']['HTTPStatusCode']}")
            raise RuntimeError(f"Bedrock API error: {e}")
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:")
            logger.error(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            logger.error(f"   ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            import traceback
            logger.error(f"   Traceback:\n{traceback.format_exc()}")
            raise


class LLMHelper:
    """LLM ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤"""
    
    @staticmethod
    def invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ê°„ë‹¨í•œ LLM í˜¸ì¶œ (Bedrock)
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        config = LLMManager.merge_config(**kwargs)
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = LLMManager._call_bedrock_converse(
            messages=messages,
            model_id=config["model_id"],
            region=config["region"],
            timeout=config["timeout"],
            temperature=kwargs.get("temperature", config["temperature"]),
            top_p=kwargs.get("top_p", config["top_p"]),
            max_tokens=kwargs.get("max_tokens", config["max_tokens"])
        )
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        output_message = response.get("output", {}).get("message", {})
        content_blocks = output_message.get("content", [])
        
        if content_blocks:
            for block in content_blocks:
                if "text" in block:
                    return block["text"]
        return ""

    
    @staticmethod
    def invoke_with_history(
        history: List[Dict[str, str]],
        tool_config: Optional[Dict] = None,
        tool_choice: Optional[Union[str, Dict]] = None,
        return_full_response: bool = False,
        **kwargs
    ) -> Union[str, Dict]:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ (Bedrock)
        
        Args:
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant/system", "content": "..."}]
            tool_config: Bedrock toolConfig (ì„ íƒ)
            tool_choice: Bedrock toolChoice (ì„ íƒ, ì˜ˆ: {"any": {}}, {"auto": {}}, {"tool": {"name": "tool_name"}})
            return_full_response: Trueë©´ ì „ì²´ ì‘ë‹µ, Falseë©´ í…ìŠ¤íŠ¸ë§Œ
            **kwargs: LLM ì„¤ì •
            
        Returns:
            str ë˜ëŠ” Dict: return_full_responseì— ë”°ë¼
        """
        config = LLMManager.merge_config(**kwargs)
        
        response = LLMManager._call_bedrock_converse(
            messages=history,
            model_id=config["model_id"],
            region=config["region"],
            timeout=config["timeout"],
            tool_config=tool_config,
            tool_choice=tool_choice,
            temperature=kwargs.get("temperature", config["temperature"]),
            top_p=kwargs.get("top_p", config["top_p"]),
            max_tokens=kwargs.get("max_tokens", config["max_tokens"])
        )
        
        # return_full_responseì— ë”°ë¼ ì²˜ë¦¬
        if return_full_response:
            return response  # ì „ì²´ ì‘ë‹µ (stopReason í¬í•¨)
        else:
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            output_message = response.get("output", {}).get("message", {})
            content_blocks = output_message.get("content", [])
            
            if content_blocks:
                for block in content_blocks:
                    if "text" in block:
                        return block["text"]
            return ""

    
    @staticmethod
    def stream_invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ (í˜„ì¬ ë¯¸ì§€ì›)
        
        Bedrock Converse ìŠ¤íŠ¸ë¦¬ë°ì€ converse_stream() API ì‚¬ìš© í•„ìš”
        í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
        """
        raise NotImplementedError("Streaming is not yet implemented for Bedrock Converse API")