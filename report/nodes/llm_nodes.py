import json
import requests
import pandas as pd
import ollama
from typing import Dict, Any

# âš ï¸ ì£¼ì˜: AgentStateëŠ” ìµœìƒìœ„ state.pyì— ì •ì˜ë˜ì–´ì•¼ í•˜ë©°, ì—¬ê¸°ì„œ ìƒëŒ€ ê²½ë¡œ importëŠ” ì‚­ì œí•©ë‹ˆë‹¤.
# ì‹¤ì œ ì‚¬ìš© ì‹œ, ê° ì—ì´ì „íŠ¸ì˜ builder.pyì—ì„œ state.pyì˜ AgentState ë˜ëŠ” í•´ë‹¹ ì—ì´ì „íŠ¸ì˜ Stateë¥¼ importí•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” íƒ€ì… íŒíŠ¸ë§Œ ì œê³µí•©ë‹ˆë‹¤.
# from state import AgentState, ConsumptionAnalysisState # ì´ ì¤„ì€ í†µí•© íŒŒì¼ì—ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.

# ==============================================================================
# ğŸ› ï¸ ê³µí†µ Ollama ì„¤ì •
# ==============================================================================
# ëª¨ë“  LLM ë…¸ë“œê°€ Ollama í˜¸ì¶œì„ ìœ„í•´ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
OLLAMA_HOST = 'http://localhost:11434' 
QWEN_MODEL = 'qwen3:8b'
# compare ì—ì´ì „íŠ¸ì—ì„œ ì‚¬ìš©ëœ ë°©ì‹:
# from llm.ollama_llm import ollama_llm  # <--- ì´ ë°©ì‹ì€ ì¤‘ì•™ ì§‘ì¤‘í™” ì‹œ ê²½ë¡œ ë¬¸ì œë¡œ ì¸í•´ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
                                        # ëŒ€ì‹ , ì•„ë˜ í•¨ìˆ˜ë“¤ì€ requests ë˜ëŠ” ollama.Clientë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.


# ==============================================================================
# 1. ğŸ” compare ì—ì´ì „íŠ¸ìš©: ë³€ë™ ì‚¬í•­ ë¹„êµ ë° ìš”ì•½ ë…¸ë“œ
# ==============================================================================
def compare_changes_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì´ì „ ë°ì´í„°ì™€ í˜„ì¬ ë³€ë™ ì‚¬í•­ì„ ë¹„êµí•˜ê³  LLMì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    Args:
        state: 'report_data', 'house_info', 'policy_info', 'credit_info' í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬.
    """
    print("ğŸ” [LLM Node] ë³€ë™ ì‚¬í•­ ë¹„êµ ë° ìš”ì•½ ì‹œì‘...")

    prompt = f"""
    ì•„ë˜ëŠ” ì´ì „ ë‹¬ ë°ì´í„°ì™€ ìƒˆë¡œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì…ë‹ˆë‹¤.
    ë³€ë™ ì‚¬í•­ì„ ê°„ê²°í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

    [ì´ì „ ë‹¬ ë°ì´í„°]: {state.get('report_data', 'ì •ë³´ ì—†ìŒ')}
    [ì£¼íƒ ì •ë³´]: {state.get('house_info', 'ì •ë³´ ì—†ìŒ')}
    [ì •ì±… ì •ë³´]: {state.get('policy_info', 'ì •ë³´ ì—†ìŒ')}
    [ì‹ ìš© ì •ë³´]: {state.get('credit_info', 'ì •ë³´ ì—†ìŒ')}
    """

    # compare ì—ì´ì „íŠ¸ì—ì„œ ì‚¬ìš©ëœ ë°©ì‹(langchain_core/ollama_llm)ì„ requests ê¸°ë°˜ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
    payload = {
        "model": "qwen3:8b", # ëª¨ë¸ì€ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½ í•„ìš” (ì˜ˆ: llama3)
        "prompt": f"[System] ë„ˆëŠ” ë°ì´í„° ë¶„ì„ê³¼ ë¦¬í¬íŠ¸ ìš”ì•½ì— ëŠ¥ìˆ™í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n\n[Human] {prompt}",
        "stream": False,
        "options": {"temperature": 0.3}
    }
    
    response_content = "âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨"
    try:
        res = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=60)
        res.raise_for_status() 
        response_content = res.json()['response'].strip()
        print("âœ… [LLM Node] ë³€ë™ ì‚¬í•­ ë¹„êµ ìš”ì•½ ì™„ë£Œ")
    except requests.exceptions.RequestException as e:
        print(f"âŒ [LLM Node] Ollama í†µì‹  ì˜¤ë¥˜: {e}")
    
    state["comparison_result"] = response_content
    return state


# ==============================================================================
# 2. ğŸ§¾ consume ì—ì´ì „íŠ¸ìš©: ìµœì¢… ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ
# ==============================================================================
def generate_final_report_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """êµ°ì§‘ ì •ë³´ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    nickname = state.get('cluster_nickname', 'ë¯¸ì • êµ°ì§‘')
    analysis_data = state.get('user_analysis', {})
    ollama_model_name = state.get('ollama_model_name', 'llama3') # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •

    # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ ë° í˜•ì‹í™”
    total_spend_amount = analysis_data.get('total_spend_amount', 'N/A')
    top_3_categories = analysis_data.get('top_3_categories', ['N/A'])
    fixed_cost = analysis_data.get('fixed_cost', 'N/A')
    non_fixed_cost_rate = analysis_data.get('non_fixed_cost_rate', 'N/A')
    
    analysis_text = (
        f"ì´ ì§€ì¶œì•¡: {total_spend_amount}, "
        f"ì£¼ ì†Œë¹„ ì˜ì—­: {', '.join(top_3_categories)}, "
        f"ê³ ì •ë¹„: {fixed_cost}, "
        f"ë¹„ê³ ì •ë¹„ ë¹„ì¤‘: {non_fixed_cost_rate}"
    )
    
    prompt_template = f"""
    [System] ë‹¹ì‹ ì€ ê³ ê°ì˜ ì†Œë¹„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê³ ê°ì—ê²Œ ì „ë‹¬í•  4~5ì¤„ì˜ **ê°„ê²°í•˜ê³  ì •ì¤‘í•œ** ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë³„ë„ì˜ ë¨¸ë¦¬ê¸€ì´ë‚˜ ê¼¬ë¦¬ê¸€ ì—†ì´ ë³¸ë¡ ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
    
    [í•µì‹¬ ì •ë³´]
    1. êµ°ì§‘ ë³„ëª…: {nickname}
    2. ê°œì¸ ë¶„ì„: {analysis_text}
    
    [ë³´ê³ ì„œ í¬í•¨ ìš”ì†Œ ë° í˜•ì‹]
    - ê³ ê°ì˜ êµ°ì§‘ ë³„ëª…ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘
    - ì£¼ ì†Œë¹„ ì˜ì—­ì„ êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ í•¨ê»˜ ì–¸ê¸‰
    - ê³ ì •ë¹„/ë¹„ê³ ì •ë¹„ ë¹„ì¤‘ì„ í•´ì„í•˜ì—¬ ì†Œë¹„ ìŠµê´€ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ í•œ ì¤„ í¬í•¨
    - ìµœì¢… ì•„ì›ƒí’‹ì€ 4~5ì¤„ì˜ ì¤„ ê¸€ í˜•íƒœì—¬ì•¼ í•¨.
    """
    
    payload = {
        "model": ollama_model_name, "prompt": prompt_template, "stream": False,
        "options": {"temperature": 0.5, "num_predict": 1024}
    }
    
    final_report = "âŒ Ollama í†µì‹  ì˜¤ë¥˜: ì„œë²„ ë¬¸ì œ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ." 
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=300) 
        response.raise_for_status() 
        final_report = response.json()['response'].strip()
        print("âœ… [LLM Node] ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    except requests.exceptions.RequestException as e:
        print(f"âŒ [LLM Node] Ollama í†µì‹  ì˜¤ë¥˜ ë°œìƒ. ì˜¤ë¥˜: {e}")
        
    state['final_report'] = final_report
    return state


# ==============================================================================
# 3. ğŸ’° profit ì—ì´ì „íŠ¸ìš©: íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ
# ==============================================================================
def generate_visualization_data(df: pd.DataFrame) -> tuple[Dict[str, float], str]:
    """ìˆ˜ìµ/ì†ì‹¤ ë°ì´í„°í”„ë ˆì„ì„ ì‹œê°í™”ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # Profit ì—ì´ì „íŠ¸ì—ì„œ LLMê³¼ í•¨ê»˜ ì‚¬ìš©ëœ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ì…ë‹ˆë‹¤.
    # LLMì´ í•„ìš”í•œ ë…¸ë“œëŠ” ì•„ë‹ˆì§€ë§Œ, í•´ë‹¹ ì—ì´ì „íŠ¸ì—ì„œ í•¨ê»˜ ì“°ì˜€ê¸°ì— í¬í•¨í•©ë‹ˆë‹¤.
    vis_data = df.groupby('type').agg({
        'principal': 'sum',
        'net_profit': 'sum',
        'net_profit_loss': 'sum'
    }).fillna(0)
    
    vis_data['total_net_p_l'] = vis_data['net_profit'] + vis_data['net_profit_loss']
    chart_data = vis_data['total_net_p_l'].to_dict()
    
    return chart_data, "" # ì°¨íŠ¸ ë°ì´í„°ì™€ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ê¸°ì¡´ í•¨ìˆ˜ì˜ í˜•íƒœ ìœ ì§€)


def analyze_investment_results_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ollamaì— êµ¬ë™ ì¤‘ì¸ Qwen LLMì„ í˜¸ì¶œí•˜ì—¬ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        state: 'analysis_df', 'total_principal', 'total_net_profit_loss' í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬.
    """
    df = state.get('analysis_df')
    total_principal = state.get('total_principal', 0)
    total_net_profit_loss = state.get('total_net_profit_loss', 0)

    # 1. ì‹œê°í™” ë°ì´í„° ì¤€ë¹„ (ê°™ì€ íŒŒì¼ ë‚´ í•¨ìˆ˜ ì‚¬ìš©)
    chart_data, _ = generate_visualization_data(df)
    
    # 2. LLMì—ê²Œ ì „ë‹¬í•  ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    financial_summary = {
        'ì´_ì›ê¸ˆ': f"{total_principal:,} ì›",
        'ì´_ìˆœìˆ˜ìµ_ì†ì‹¤': f"{total_net_profit_loss:,.0f} ì›",
        'ìˆ˜ìµë¥ ': f"{total_net_profit_loss / total_principal * 100:.2f}%" if total_principal > 0 else "0.00%",
        'ìƒí’ˆë³„_ìƒì„¸': df.to_dict('records'),
        'ì‹œê°í™”_ë°ì´í„°': chart_data
    }
    
    # 3. Ollama í´ë¼ì´ì–¸íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    client = ollama.Client(host=OLLAMA_HOST)

    prompt = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ JSON í˜•ì‹ì˜ íˆ¬ì ìš”ì•½ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    ë³´ê³ ì„œì—ëŠ” ë‹¤ìŒ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
    1. ì´ íˆ¬ì ì›ê¸ˆ ëŒ€ë¹„ ìµœì¢… ìˆœìˆ˜ìµ/ì†ì‹¤ ìš”ì•½.
    2. ê°€ì¥ í° ìˆ˜ìµì„ ë‚¸ ìƒí’ˆ íƒ€ì…ê³¼ ê°€ì¥ í° ì†ì‹¤ì„ ë‚¸ ìƒí’ˆ íƒ€ì… ë¶„ì„.
    3. ì „ì²´ì ì¸ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¡°ì–¸.
    
    [íˆ¬ì ìš”ì•½ ë°ì´í„° (JSON)]
    {json.dumps(financial_summary, indent=2, ensure_ascii=False)}
    """
    
    llm_analysis_result = "Ollama Qwen í˜¸ì¶œ ì‹¤íŒ¨ (Ollama ì„œë²„ ì‹¤í–‰, ëª¨ë¸ëª…, ë„¤íŠ¸ì›Œí¬ í™•ì¸ í•„ìš”)"
    
    try:
        response = client.generate(
            model=QWEN_MODEL,
            prompt=prompt
        )
        llm_analysis_result = response['response'].strip()
        print("âœ… [LLM Node] íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            
    except Exception as e:
        llm_analysis_result = f"âŒ [LLM Node] Ollama í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. Ollama ì„œë²„ê°€ {OLLAMA_HOST}ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    
    state['investment_analysis_result'] = llm_analysis_result
    return state

# LLMì´ í•„ìš”í•œ ë…¸ë“œ í•¨ìˆ˜:
# - compare_changes_node
# - generate_final_report_node
# - analyze_investment_results_node
# - generate_visualization_data (profit ì—ì´ì „íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ìš©)