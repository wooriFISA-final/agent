"""
LLM Manager Module
LLM ì„¤ì • ë° ê´€ë¦¬ (Ollama HTTP API ì§ì ‘ í˜¸ì¶œ, LangChain ì˜ì¡´ì„± ì œê±°)
"""
from typing import Optional, Dict, Any, List
import requests
from core.logging.logger import setup_logger

logger = setup_logger()


class LLMManager:
    """
    LLM ê´€ë¦¬ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤)
    Ollama Chat APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ë°©ì‹ (LangChain ë¶ˆí•„ìš”)
    """
    
    _instance: Optional['LLMManager'] = None
    _config: Optional[Dict[str, Any]] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def get_config(cls, **kwargs) -> Dict[str, Any]:
        """
        LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        
        Args:
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        if cls._config is None:
            cls._config = cls._create_config(**kwargs)
        return cls._config
    
    @classmethod
    def _create_config(cls, **kwargs) -> Dict[str, Any]:
        """LLM ì„¤ì • ìƒì„±"""
        provider = kwargs.get("provider", "ollama")
        model = kwargs.get("model", "qwen3:8b")
        temperature = kwargs.get("temperature", 0.3)
        base_url = kwargs.get("base_url", "http://localhost:11434")
        timeout = kwargs.get("timeout", 180)
        
        llm_config = {
            "provider": provider,
            "model": model,
            "temperature": temperature,
            "base_url": base_url,
            "timeout": timeout
        }
        
        logger.info(f"ğŸ¤– LLM Config: provider={provider}, model={model}, base_url={base_url}")
        return llm_config
    
    @classmethod
    def reset(cls):
        """LLM ì„¤ì • ì´ˆê¸°í™”"""
        cls._config = None
        logger.info("ğŸ”„ LLM config reset")
    
    @classmethod
    def test_connection(cls) -> bool:
        """
        LLM ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            config = cls.get_config()
            response = cls._call_ollama_chat(
                messages=[{"role": "user", "content": "Hello"}],
                model=config["model"],
                base_url=config["base_url"],
                timeout=10,
                temperature=0.1,
                stream=False
            )
            logger.info(f"âœ… LLM connection test successful")
            return True
        except Exception as e:
            logger.error(f"âŒ LLM connection test failed: {e}")
            return False
    
    @classmethod
    def _call_ollama_chat(
        cls,
        messages: List[Dict[str, str]],
        model: str,
        base_url: str,
        timeout: int = 180,
        temperature: float = 0.3,
        stream: bool = False,
        format: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Ollama Chat API í˜¸ì¶œ
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user/assistant/system", "content": "..."}]
            model: ëª¨ë¸ ì´ë¦„
            base_url: Ollama ì„œë²„ URL
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            temperature: ì˜¨ë„ ì„¤ì •
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            format: ì‘ë‹µ í¬ë§· (json ë“±)
            options: ì¶”ê°€ ì˜µì…˜
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "options": options or {"temperature": temperature}
        }
        
        if format:
            payload["format"] = format
        
        try:
            response = requests.post(
                f"{base_url}/api/chat",
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            
            result = response.json()
            
            logger.info(f"chat ollama result : {result}")
            # stream=Falseì¸ ê²½ìš° message.content ë°˜í™˜
            if not stream:
                return result.get('message', {}).get('content', '').strip()
            
            # stream=Trueì¸ ê²½ìš°ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
            return result
            
        except requests.exceptions.Timeout:
            error_msg = f"âŒ Ollama íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ ì´ˆê³¼). ëª¨ë¸: {model}"
            logger.error(error_msg)
            raise TimeoutError(error_msg)
            
        except requests.exceptions.ConnectionError:
            error_msg = f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {base_url}"
            logger.error(error_msg)
            raise ConnectionError(error_msg)
            
        except requests.exceptions.RequestException as e:
            error_msg = f"âŒ Ollama API í˜¸ì¶œ ì˜¤ë¥˜: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)


class LLMHelper:
    """LLM ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤ (LangChain ë¶ˆí•„ìš”)"""
    
    @staticmethod
    def invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ê°„ë‹¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            **kwargs: LLM ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        config = LLMManager.get_config(**kwargs)
        
        # kwargsì—ì„œ ì„¤ì • ì¶”ì¶œ
        model = kwargs.get("model", config["model"])
        base_url = kwargs.get("base_url", config["base_url"])
        temperature = kwargs.get("temperature", config["temperature"])
        timeout = kwargs.get("timeout", config["timeout"])
        format_type = kwargs.get("format")
        options = kwargs.get("options")
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        return LLMManager._call_ollama_chat(
            messages=messages,
            model=model,
            base_url=base_url,
            timeout=timeout,
            temperature=temperature,
            stream=False,
            format=format_type,
            options=options
        )
    
    @staticmethod
    def invoke_with_history(
        prompt: str,
        history: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ LLM í˜¸ì¶œ
        
        Args:
            prompt: í˜„ì¬ í”„ë¡¬í”„íŠ¸
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant/system", "content": "..."}]
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Returns:
            LLM ì‘ë‹µ
        """
        config = LLMManager.get_config(**kwargs)
        
        model = kwargs.get("model", config["model"])
        base_url = kwargs.get("base_url", config["base_url"])
        temperature = kwargs.get("temperature", config["temperature"])
        timeout = kwargs.get("timeout", config["timeout"])
        format_type = kwargs.get("format")
        options = kwargs.get("options")
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        messages.extend(history)
        
        # í˜„ì¬ í”„ë¡¬í”„íŠ¸
        messages.append({"role": "user", "content": prompt})
        
        return LLMManager._call_ollama_chat(
            messages=messages,
            model=model,
            base_url=base_url,
            timeout=timeout,
            temperature=temperature,
            stream=False,
            format=format_type,
            options=options
        )
    
    @staticmethod
    def stream_invoke(
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            **kwargs: LLM ì„¤ì •
            
        Yields:
            ì‘ë‹µ ì²­í¬
        """
        config = LLMManager.get_config(**kwargs)
        
        model = kwargs.get("model", config["model"])
        base_url = kwargs.get("base_url", config["base_url"])
        temperature = kwargs.get("temperature", config["temperature"])
        timeout = kwargs.get("timeout", config["timeout"])
        format_type = kwargs.get("format")
        options = kwargs.get("options")
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "options": options or {"temperature": temperature}
        }
        
        if format_type:
            payload["format"] = format_type
        
        try:
            response = requests.post(
                f"{base_url}/api/chat",
                json=payload,
                timeout=timeout,
                stream=True
            )
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    import json
                    chunk = json.loads(line)
                    message = chunk.get('message', {})
                    content = message.get('content', '')
                    if content:
                        yield content
                        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            raise