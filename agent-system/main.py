"""
api_server.py
Front â†” Agent System FastAPI API Server
- main.pyëŠ” ê°œë°œìš©, api_serverëŠ” í”„ë¡ íŠ¸ í†µì‹ ìš©
"""

import asyncio
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.messages import HumanMessage

from agents.registry.agent_registry import AgentRegistry
from graph.builder.graph_builder import GraphBuilder
from core.llm.llm_manger import LLMManager
from core.logging.logger import setup_logger

# ----------------------------
# ê¸°ë³¸ ì„¤ì •
# ----------------------------
app = FastAPI(title="Agent System API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œí•˜ëŠ” ê²Œ ì¢‹ì•„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----------------------------
# ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
# ----------------------------
from typing import TypedDict, Optional

class LLMStateSchema(TypedDict, total=False):
    query: str
    research_result: Optional[str]
    analysis_result: Optional[str]
    final_report: Optional[str]
    intent_result: Optional[str]
    messages: Optional[str]


# ----------------------------
# ê·¸ë˜í”„ ì´ˆê¸°í™”
# ----------------------------
logger = setup_logger()
logger.info("ğŸš€ Initializing LLM Graph...")

AgentRegistry.auto_discover("agents.implementations")
registered_agents = AgentRegistry.list_agents()
logger.info(f"âœ… Registered agents: {registered_agents}")

builder = GraphBuilder(LLMStateSchema)
llm_agent_config = {"timeout": 120, "max_retries": 2}

if "intent_classifier" in registered_agents:
    builder.add_agent_node("intent", "intent_classifier", config=llm_agent_config)
    builder.set_entry_point("intent")
    builder.set_finish_point("intent")
    graph = builder.build()
    logger.info("âœ… Graph successfully built and ready to use.")
else:
    logger.error("âŒ Required agent 'intent_classifier' not found. Please add it.")
    graph = None


# ----------------------------
# Request/Response ëª¨ë¸
# ----------------------------
class ChatRequest(BaseModel):
    message: str
    session_id: str = "default-session"


class ChatResponse(BaseModel):
    response: str


# ----------------------------
# Chat API Endpoint
# ----------------------------
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Front â†’ LLM Graph â†’ Response"""
    if not graph:
        return ChatResponse(response="âŒ Agent graph is not initialized properly.")

    config = {"configurable": {"thread_id": request.session_id}}
    # # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    # initial_state = {
    #     "query": "ê³„íšì„ ìˆ˜ì •í•˜ê³  ì‹¶ì–´"
    # }
    try:
        result = await graph.ainvoke({"query": request.message}, config=config)
        print(result)
        # result = await graph.ainvoke({"messages": [HumanMessage(content=request.message)]}, config=config)

        final_response = result.get("messages")

        if not final_response:
            return ChatResponse(response="ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if isinstance(final_response, list):
            # Simple join for now.
            final_response = " ".join(map(str, final_response))

        return ChatResponse(response=final_response)

    except Exception as e:
        logger.error(f"âŒ Chat processing failed: {e}")
        return ChatResponse(response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# ----------------------------
# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
# ----------------------------
@app.get("/")
async def root():
    return {"status": "ok", "message": "AI Agent API is running ğŸš€"}


# ----------------------------
# ì„œë²„ ì§ì ‘ ì‹¤í–‰ìš© (ì„ íƒ)
# ----------------------------
if __name__ == "__main__":
    import uvicorn

    logger.info("ğŸš€ Starting API Server on http://localhost:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)
