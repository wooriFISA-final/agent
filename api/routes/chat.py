"""
ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸

ì‚¬ìš©ìì™€ AI ê°„ì˜ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""
from fastapi import APIRouter, Request
from langchain_core.messages import HumanMessage, AIMessage
import asyncio
from typing import Dict

from core.logging.logger import setup_logger
from core.config.setting import settings
from agents.config.base_config import StateBuilder
from api.models import ChatRequest, ChatResponse

logger = setup_logger()

router = APIRouter()

# ì„¸ì…˜ë³„ ì ê¸ˆ ì €ì¥ì†Œ (ë™ì¼ ì„¸ì…˜ì˜ ë™ì‹œ ìš”ì²­ ë°©ì§€)
_session_locks: Dict[str, asyncio.Lock] = {}




async def _execute_graph(
    request: Request,
    chat_request: ChatRequest,
    graph_name: str = "default"
) -> ChatResponse:
    """ê·¸ë˜í”„ ì‹¤í–‰ ê³µí†µ ë¡œì§
    
    Args:
        request: FastAPI Request ê°ì²´
        chat_request: ì±„íŒ… ìš”ì²­ ë°ì´í„°
        graph_name: ì‚¬ìš©í•  ê·¸ë˜í”„ ì´ë¦„
        
    Returns:
        ChatResponse: AI ì‘ë‹µ ë°ì´í„°
    """
    session_id = chat_request.session_id
    
    # ì„¸ì…˜ë³„ ì ê¸ˆ ìƒì„± (ì—†ìœ¼ë©´)
    if session_id not in _session_locks:
        _session_locks[session_id] = asyncio.Lock()
    
    # ğŸ”’ ì„¸ì…˜ ì ê¸ˆ íšë“ (ë™ì¼ ì„¸ì…˜ì˜ ë‹¤ë¥¸ ìš”ì²­ì€ ëŒ€ê¸°)
    async with _session_locks[session_id]:
        logger.info(f"ğŸ”’ Session lock acquired for '{session_id}'")
        
        graph = request.app.state.get_graph(graph_name)
        if not graph:
            logger.error(f"âŒ Graph '{graph_name}' not initialized")
            available_graphs = request.app.state.list_graphs()
            return ChatResponse(
                response=f"Graph '{graph_name}' is not available. Available graphs: {available_graphs}",
                status="error",
                metadata={
                    "error": "graph_not_found",
                    "graph": graph_name,
                    "available_graphs": available_graphs
                }
            )

    try:
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“© NEW REQUEST | Graph: {graph_name} | Session: {chat_request.session_id}")
        logger.info(f"   Message: {chat_request.message}")
        logger.info(f"{'='*80}")

        graph_config = {"configurable": {"thread_id": chat_request.session_id}}

        # Check for existing conversation state
        try:
            existing_state = await graph.aget_state(graph_config)
            has_history = existing_state and existing_state.values.get('global_messages')
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load existing state for session '{chat_request.session_id}': {e}")
            has_history = False

        if has_history:
            logger.info(f"ğŸ“š Continuing conversation for session '{chat_request.session_id}'")
            input_state = {"global_messages": [HumanMessage(content=chat_request.message)]}
        else:
            logger.info(f"ğŸ†• Starting new conversation for session '{chat_request.session_id}'")
            input_state = StateBuilder.create_initial_state(
                messages=[HumanMessage(content=chat_request.message)],
                session_id=chat_request.session_id,
                max_iterations=settings.MAX_GRAPH_ITERATIONS
            )

        # Execute the agent graph
        logger.info(f"ğŸš€ Executing '{graph_name}' graph...")
        result_state = await graph.ainvoke(input_state, config=graph_config)
        logger.info("âœ… Graph execution completed.")

        # Extract the final response from global_messages
        all_messages = result_state.get("global_messages", [])
        ai_messages = [m for m in all_messages if isinstance(m, AIMessage)]

        if not ai_messages:
            logger.warning("âš ï¸ No AI messages found in the final state.")
            # í´ë°±: last_result í™•ì¸
            last_result = result_state.get("last_result")
            if last_result:
                logger.info("ğŸ“Œ Using last_result as fallback response")
                return ChatResponse(
                    response=last_result,
                    status="success",
                    metadata={
                        "session_id": chat_request.session_id,
                        "graph": graph_name,
                        "source": "last_result"
                    }
                )
            return ChatResponse(
                response="AI did not generate a response.",
                status="warning",
                metadata={"graph": graph_name}
            )

        final_response = ai_messages[-1].content
        logger.info(f"ğŸ’¬ Returning response for session '{chat_request.session_id}'.")
        logger.info(f"ğŸ”“ Session lock will be released for '{session_id}'")

        return ChatResponse(
            response=final_response,
            status="success",
            metadata={
                "session_id": chat_request.session_id,
                "graph": graph_name
            }
        )

    except asyncio.TimeoutError:
        logger.error(f"âŒ Request timeout for session '{chat_request.session_id}'")
        logger.info(f"ğŸ”“ Session lock will be released for '{session_id}'")
        return ChatResponse(
            response="Request timed out.",
            status="error",
            metadata={
                "error": "timeout",
                "session_id": chat_request.session_id,
                "graph": graph_name
            }
        )
    
    except Exception as e:
        logger.error(f"âŒ Chat processing failed for session '{chat_request.session_id}': {e}", exc_info=True)
        logger.info(f"ğŸ”“ Session lock will be released for '{session_id}'")
        return ChatResponse(
            response=f"An internal error occurred: {str(e)}",
            status="error",
            metadata={
                "error": "processing_error",
                "detail": str(e),
                "session_id": chat_request.session_id,
                "graph": graph_name
            }
        )


@router.post("/chat/plan", response_model=ChatResponse)
async def chat_plan_endpoint(request: Request, chat_request: ChatRequest):
    """Plan ê·¸ë˜í”„ ì „ìš© ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    
    ì¬ë¬´ ê³„íš ê´€ë ¨ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        request: FastAPI Request ê°ì²´
        chat_request: ì±„íŒ… ìš”ì²­ ë°ì´í„°
        
    Returns:
        ChatResponse: AI ì‘ë‹µ ë°ì´í„°
    """
    return await _execute_graph(request, chat_request, "plan")


@router.post("/chat/report", response_model=ChatResponse)
async def chat_report_endpoint(request: Request, chat_request: ChatRequest):
    """Report ê·¸ë˜í”„ ì „ìš© ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    
    ë¦¬í¬íŠ¸ ìƒì„± ê´€ë ¨ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        request: FastAPI Request ê°ì²´
        chat_request: ì±„íŒ… ìš”ì²­ ë°ì´í„°
        
    Returns:
        ChatResponse: AI ì‘ë‹µ ë°ì´í„°
    """
    return await _execute_graph(request, chat_request, "report")
