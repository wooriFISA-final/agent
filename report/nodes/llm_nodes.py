# report_project/report/nodes/llm_nodes.py

import json
import requests
import pandas as pd
import ollama
from typing import Dict, Any

# ==============================================================================
# ğŸ› ï¸ ê³µí†µ Ollama ì„¤ì •
# ==============================================================================
OLLAMA_HOST = 'http://localhost:11434' 
QWEN_MODEL = 'qwen3:8b'


# ==============================================================================
# 1. ğŸ” compare ì—ì´ì „íŠ¸ìš©: ë³€ë™ ì‚¬í•­ ë¹„êµ ë° ìš”ì•½ ë…¸ë“œ (RAG ê¸°ë°˜)
# ==============================================================================
def compare_changes_node(state: Dict[str, Any]) -> Dict[str, Any]:
    print("ğŸ” ë³€ë™ ì‚¬í•­ ë¹„êµ ë° ìš”ì•½ ì‹œì‘...")
    
    # ğŸš¨ ìˆ˜ì •: policy_infoì—ì„œ old_policyì™€ new_policyë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    policy_data = state.get('policy_info', {})
    old_policies = policy_data.get('old_policy', [])
    new_policies = policy_data.get('new_policy', [])
    
    # 1. ê²€ìƒ‰ëœ ì²­í¬ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
    context_text = "\n\n--- [RAG ê²€ìƒ‰ ê²°ê³¼: ì •ì±… ë³€ë™ ì»¨í…ìŠ¤íŠ¸] ---\n"
    
    # ğŸš¨ [ìˆ˜ì • í•µì‹¬] old_policies (ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸) ë‚´ìš© í¬ë§·íŒ…
    if old_policies:
        context_text += "--- ì´ì „ ì •ì±… (20241224) ì²­í¬ ---\n"
        for i, content in enumerate(old_policies):
            # contentëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ, .get() ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
            context_text += f"[ì´ì „ ì •ì±… ì²­í¬ {i+1}]\në‚´ìš©: {content[:300]}...\n---\n" 
    
    # ğŸš¨ [ìˆ˜ì • í•µì‹¬] new_policies (ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸) ë‚´ìš© í¬ë§·íŒ…
    if new_policies:
        context_text += "\n--- ì‹ ê·œ ì •ì±… (20250305) ì²­í¬ ---\n"
        for i, content in enumerate(new_policies):
            # contentëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ, .get() ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
            context_text += f"[ì‹ ê·œ ì •ì±… ì²­í¬ {i+1}]\në‚´ìš©: {content[:300]}...\n---\n"
    
    if not old_policies and not new_policies:
        context_text += "ì •ì±… ë³€ë™ ë¶„ì„ì„ ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."


    prompt = f"""
    ë‹¹ì‹ ì€ ê¸ˆìœµ ì •ì±… ë¹„êµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [RAG ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸] í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤. ì´ ì»¨í…ìŠ¤íŠ¸ì—ëŠ” 2024ë…„ 12ì›” ë²„ì „ê³¼ 2025ë…„ 3ì›” ë²„ì „ì˜ ì •ì±… ì¡°í•­ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    [í•µì‹¬ ì„ë¬´: ì˜¤ì§ ë³€ê²½ì ë§Œ ì¶”ì¶œ]
    1. **ë‘ ì •ì±…ì˜ ëª¨ë“  [ì¥ ì œëª©]ì„ ëŒ€ì¡°**í•˜ì—¬, **ì‹ ê·œ ì •ì±…(20250305)ì—ì„œ ë³€ê²½ë˜ê±°ë‚˜ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ë‚´ìš©**ë§Œì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë³´ê³ í•˜ì‹­ì‹œì˜¤.
    2. ë³€ê²½ì´ ì—†ëŠ” ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    3. ì •ì±… íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆë‹¤ë©´ (ë‚´ìš©ì— 'ì •ì±… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨' í¬í•¨) í•´ë‹¹ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ê³  ë¶„ì„ì„ ì¤‘ë‹¨í•˜ì‹­ì‹œì˜¤.
    
    {context_text}
    
    [ì´ì „ ë‹¬ ì¬ë¬´/í™˜ê²½ ë°ì´í„° - ë¶„ì„ ì°¸ê³ ìš©]
    - ì´ì „ ë‹¬ ë³´ê³ ì„œ: {state.get('report_data', 'N/A')}
    - í˜„ì¬ ì£¼íƒ ì •ë³´: {state.get('house_info', 'N/A')}
    - í˜„ì¬ ì‹ ìš© ì •ë³´: {state.get('credit_info', 'N/A')}
    """
    
    # ğŸš¨ Ollama í˜¸ì¶œ ë¡œì§ (íƒ€ì„ì•„ì›ƒ 180ì´ˆë¡œ ì„¤ì •)
    response_content = "âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨"
    payload = {
        "model": QWEN_MODEL, 
        "prompt": f"[System] ë„ˆëŠ” ë°ì´í„° ë¶„ì„ê³¼ ë¦¬í¬íŠ¸ ìš”ì•½ì— ëŠ¥ìˆ™í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n\n[Human] {prompt}",
        "stream": False,
        "options": {"temperature": 0.3}
    }
    
    try:
        res = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180)
        res.raise_for_status() 
        response_content = res.json()['response'].strip()
        print("âœ… [LLM Node] ë³€ë™ ì‚¬í•­ ë¹„êµ ìš”ì•½ ì™„ë£Œ")
    except requests.exceptions.RequestException as e:
        response_content = f"âŒ [LLM Node] Ollama í†µì‹  ì˜¤ë¥˜: {e}. Ollama ì„œë²„(http://localhost:11434)ì™€ ëª¨ë¸({QWEN_MODEL}) ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
        print(response_content)
    
    state["comparison_result"] = response_content
    return state


# ==============================================================================
# 2. ğŸ§¾ consume ì—ì´ì „íŠ¸ìš©: ìµœì¢… ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ
# ==============================================================================
def generate_final_report_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """êµ°ì§‘ ì •ë³´ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    nickname = state.get('cluster_nickname', 'ë¯¸ì • êµ°ì§‘')
    analysis_data = state.get('user_analysis', {})
    ollama_model_name = state.get('ollama_model_name', 'llama3') # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •

    # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ ë° í˜•ì‹í™”
    total_spend_amount = analysis_data.get('total_spend_amount', 'N/A')
    top_3_categories = analysis_data.get('top_3_categories', ['N/A'])
    fixed_cost = analysis_data.get('fixed_cost', 'N/A')
    non_fixed_cost_rate = analysis_data.get('non_fixed_cost_rate', 'N/A')
    
    analysis_text = (
        f"ì´ ì§€ì¶œì•¡: {total_spend_amount}, "
        f"ì£¼ ì†Œë¹„ ì˜ì—­: {', '.join(top_3_categories)}, "
        f"ê³ ì •ë¹„: {fixed_cost}, "
        f"ë¹„ê³ ì •ë¹„ ë¹„ì¤‘: {non_fixed_cost_rate}"
    )
    
    prompt_template = f"""
    [System] ë‹¹ì‹ ì€ ê³ ê°ì˜ ì†Œë¹„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê³ ê°ì—ê²Œ ì „ë‹¬í•  4~5ì¤„ì˜ **ê°„ê²°í•˜ê³  ì •ì¤‘í•œ** ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë³„ë„ì˜ ë¨¸ë¦¬ê¸€ì´ë‚˜ ê¼¬ë¦¬ê¸€ ì—†ì´ ë³¸ë¡ ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
    
    [í•µì‹¬ ì •ë³´]
    1. êµ°ì§‘ ë³„ëª…: {nickname}
    2. ê°œì¸ ë¶„ì„: {analysis_text}
    
    [ë³´ê³ ì„œ í¬í•¨ ìš”ì†Œ ë° í˜•ì‹]
    - ê³ ê°ì˜ êµ°ì§‘ ë³„ëª…ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘
    - ì£¼ ì†Œë¹„ ì˜ì—­ì„ êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ í•¨ê»˜ ì–¸ê¸‰
    - ê³ ì •ë¹„/ë¹„ê³ ì •ë¹„ ë¹„ì¤‘ì„ í•´ì„í•˜ì—¬ ì†Œë¹„ ìŠµê´€ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ í•œ ì¤„ í¬í•¨
    - ìµœì¢… ì•„ì›ƒí’‹ì€ 4~5ì¤„ì˜ ì¤„ ê¸€ í˜•íƒœì—¬ì•¼ í•¨.
    """
    
    payload = {
        "model": ollama_model_name, "prompt": prompt_template, "stream": False,
        "options": {"temperature": 0.5, "num_predict": 1024}
    }
    
    final_report = "âŒ Ollama í†µì‹  ì˜¤ë¥˜: ì„œë²„ ë¬¸ì œ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ." 
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=300) 
        response.raise_for_status() 
        final_report = response.json()['response'].strip()
        print("âœ… [LLM Node] ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    except requests.exceptions.RequestException as e:
        print(f"âŒ [LLM Node] Ollama í†µì‹  ì˜¤ë¥˜ ë°œìƒ. ì˜¤ë¥˜: {e}")
        
    state['final_report'] = final_report
    return state


# ==============================================================================
# 3. ğŸ’° profit ì—ì´ì „íŠ¸ìš©: íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ
# ==============================================================================
def generate_visualization_data(df: pd.DataFrame) -> tuple[Dict[str, float], str]:
    """ìˆ˜ìµ/ì†ì‹¤ ë°ì´í„°í”„ë ˆì„ì„ ì‹œê°í™”ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # Profit ì—ì´ì „íŠ¸ì—ì„œ LLMê³¼ í•¨ê»˜ ì‚¬ìš©ëœ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ì…ë‹ˆë‹¤.
    # LLMì´ í•„ìš”í•œ ë…¸ë“œëŠ” ì•„ë‹ˆì§€ë§Œ, í•´ë‹¹ ì—ì´ì „íŠ¸ì—ì„œ í•¨ê»˜ ì“°ì˜€ê¸°ì— í¬í•¨í•©ë‹ˆë‹¤.
    vis_data = df.groupby('type').agg({
        'principal': 'sum',
        'net_profit': 'sum',
        'net_profit_loss': 'sum'
    }).fillna(0)
    
    vis_data['total_net_p_l'] = vis_data['net_profit'] + vis_data['net_profit_loss']
    chart_data = vis_data['total_net_p_l'].to_dict()
    
    return chart_data, "" # ì°¨íŠ¸ ë°ì´í„°ì™€ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ê¸°ì¡´ í•¨ìˆ˜ì˜ í˜•íƒœ ìœ ì§€)


def analyze_investment_results_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ollamaì— êµ¬ë™ ì¤‘ì¸ Qwen LLMì„ í˜¸ì¶œí•˜ì—¬ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        state: 'analysis_df', 'total_principal', 'total_net_profit_loss' í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬.
    """
    df = state.get('analysis_df')
    total_principal = state.get('total_principal', 0)
    total_net_profit_loss = state.get('total_net_profit_loss', 0)

    # 1. ì‹œê°í™” ë°ì´í„° ì¤€ë¹„ (ê°™ì€ íŒŒì¼ ë‚´ í•¨ìˆ˜ ì‚¬ìš©)
    chart_data, _ = generate_visualization_data(df)
    
    # 2. LLMì—ê²Œ ì „ë‹¬í•  ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    financial_summary = {
        'ì´_ì›ê¸ˆ': f"{total_principal:,} ì›",
        'ì´_ìˆœìˆ˜ìµ_ì†ì‹¤': f"{total_net_profit_loss:,.0f} ì›",
        'ìˆ˜ìµë¥ ': f"{total_net_profit_loss / total_principal * 100:.2f}%" if total_principal > 0 else "0.00%",
        'ìƒí’ˆë³„_ìƒì„¸': df.to_dict('records'),
        'ì‹œê°í™”_ë°ì´í„°': chart_data
    }
    
    # 3. Ollama í´ë¼ì´ì–¸íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    client = ollama.Client(host=OLLAMA_HOST)

    prompt = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ JSON í˜•ì‹ì˜ íˆ¬ì ìš”ì•½ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    ë³´ê³ ì„œì—ëŠ” ë‹¤ìŒ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
    1. ì´ íˆ¬ì ì›ê¸ˆ ëŒ€ë¹„ ìµœì¢… ìˆœìˆ˜ìµ/ì†ì‹¤ ìš”ì•½.
    2. ê°€ì¥ í° ìˆ˜ìµì„ ë‚¸ ìƒí’ˆ íƒ€ì…ê³¼ ê°€ì¥ í° ì†ì‹¤ì„ ë‚¸ ìƒí’ˆ íƒ€ì… ë¶„ì„.
    3. ì „ì²´ì ì¸ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¡°ì–¸.
    
    [íˆ¬ì ìš”ì•½ ë°ì´í„° (JSON)]
    {json.dumps(financial_summary, indent=2, ensure_ascii=False)}
    """
    
    llm_analysis_result = "Ollama Qwen í˜¸ì¶œ ì‹¤íŒ¨ (Ollama ì„œë²„ ì‹¤í–‰, ëª¨ë¸ëª…, ë„¤íŠ¸ì›Œí¬ í™•ì¸ í•„ìš”)"
    
    try:
        response = client.generate(
            model=QWEN_MODEL,
            prompt=prompt
        )
        llm_analysis_result = response['response'].strip()
        print("âœ… [LLM Node] íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            
    except Exception as e:
        llm_analysis_result = f"âŒ [LLM Node] Ollama í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. Ollama ì„œë²„ê°€ {OLLAMA_HOST}ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    
    state['investment_analysis_result'] = llm_analysis_result
    return state