import asyncio
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.messages import HumanMessage

from graph.schemas.state import LLMStateSchema
from agents.registry.agent_registry import AgentRegistry
from graph.builder.graph_builder import GraphBuilder
from core.llm.llm_manger import LLMManager
from core.logging.logger import setup_logger
from graph.factory import mk_graph

from fastmcp import Client
from fastmcp.client.transports import StreamableHttpTransport

logger = setup_logger()
app = FastAPI(title="Multi-Agent Planner")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# === MCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ì •ì˜ ì¶”í›„ì— ë‹¤ë¥¸ ì½”ë“œ íŒŒì¼ì— ì˜®ê¸¸ ì˜ˆì •===
transport = StreamableHttpTransport(
    url="http://localhost:8888/mcp/"
    #headers={"X-Account-Password": "1234"}
)
mcp_client = Client(transport)


# ê·¸ë˜í”„ ì´ˆê¸°í™”
# graph = create_graph()
# graph = mk_graph("graph.yaml")  # UserRegistrationAgent í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨

AgentRegistry.auto_discover("agents.implementations")
logger.info(AgentRegistry.list_agents())
# ê·¸ë˜í”„ ë¹Œë“œ
builder = GraphBuilder(LLMStateSchema)
builder.add_agent_node("user_regri", "user_registration") \
    .set_entry_point("user_regri") \
    .set_finish_point("user_regri")

graph = builder.build()


class ChatRequest(BaseModel):
    message: str
    session_id: str = "default-session"

class ChatResponse(BaseModel):
    response: str

#ì˜ˆë„ ë³€ê²½í•´ì•¼ í• ë“¯
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Front â†’ LLM Graph â†’ Response"""
    if not graph:
        return ChatResponse(response="âŒ Agent graph is not initialized properly.")

    config = {"configurable": {"thread_id": request.session_id}}
    # # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    # initial_state = {
    #     "query": "ê³„íšì„ ìˆ˜ì •í•˜ê³  ì‹¶ì–´"
    # }
    try:
        logger.info(f"ìœ ì € ë©”ì‹œì§€ request : {request.message}")
        messages = [HumanMessage(content=request.message)]
        result = await graph.ainvoke(
            {"messages": messages},
            config=config
        )
        # result = await graph.ainvoke({"messages": [HumanMessage(content=request.message)]}, config=config)

        final_response = result.get("messages")

        if not final_response:
            return ChatResponse(response="ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if isinstance(final_response, list):
            # Simple join for now.
            final_response = " ".join(map(str, final_response))

        return ChatResponse(response=final_response)

    except Exception as e:
        logger.error(f"âŒ Chat processing failed: {e}")
        return ChatResponse(response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
@app.get("/")
async def root():
    return {"status": "ok", "message": "AI Agent API is running ğŸš€"}

# ----------------------------
# ì„œë²„ ì§ì ‘ ì‹¤í–‰ìš© (ì„ íƒ)
# ----------------------------
if __name__ == "__main__":
    import uvicorn

    logger.info("ğŸš€ Starting API Server on http://localhost:8080")
    uvicorn.run("main:app", host="0.0.0.0", port=8080, reload=True)